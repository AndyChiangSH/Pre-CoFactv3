batch_size: 12
device: '1'
dropout: 0.1
epochs: 10
eval_per_epochs: 1
freeze_text: 1
gpu: true
head: 2
hidden_dim: 256
loss_weight: 0.7
lr: 5.0e-05
max_sequence_length: 512
output_folder_name: ./model/20231101-222925_Llama-2-7b-hf/
pretrained_text: meta-llama/Llama-2-7b-hf
qa_dim: 4096
seed_value: 42
text_dim: 4096
text_model_gpu: false
